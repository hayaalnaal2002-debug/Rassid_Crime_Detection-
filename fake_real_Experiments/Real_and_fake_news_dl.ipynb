{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkpfwMsFmZMB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4P8_cYXnIoJ",
        "outputId": "29dd4dc4-b863-4459-f2dc-70d9b0bda36b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/New WinRAR ZIP archive.zip\n",
            "replace fake_news_data _finall.csv - fake_news_data (1).csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: fake_news_data _finall.csv - fake_news_data (1).csv  \n",
            "  inflating: merged_dataset12 - merged_dataset1.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/drive/MyDrive/New WinRAR ZIP archive.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VtxdKb2mnhj5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file_path = '/content/fake_news_data _finall.csv - fake_news_data (1).csv'\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" Total rows: {len(df)}\")\n",
        "print(f\" Column names: {df.columns.tolist()}\")\n",
        "\n",
        "label_col = 'label'\n",
        "print(\" Real vs Fake Count:\")\n",
        "print(df['Label'].value_counts())\n",
        "if label_col in df.columns:\n",
        "    print(\"\\n Label distribution:\")\n",
        "    print(df[label_col].value_counts())\n",
        "else:\n",
        "    print(f\" Column '{label_col}' not found. Please check column names above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOHNmNN3_Z83",
        "outputId": "c48d1766-c737-4cd3-9b4b-a0535f30fb0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total rows: 44151\n",
            " Column names: ['Label', 'Topic', 'Article_content']\n",
            " Real vs Fake Count:\n",
            "Label\n",
            "fake    40297\n",
            "real     3854\n",
            "Name: count, dtype: int64\n",
            " Column 'label' not found. Please check column names above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJRGCetqwSVY"
      },
      "source": [
        "## Ø¨ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø§Ù…ÙˆØ¯ Ø§Ù„ØªÙˆØ¨ÙŠÙƒ Ø´Ùˆ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ù‚Ù„Ø¨ÙˆØ§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sa_SUp0nyqV",
        "outputId": "66568007-b0c7-418f-a442-ccd3abdafefe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic\n",
            "Ø±ÙŠØ§Ø¶Ø©                     16984\n",
            "Ø³ÙŠØ§Ø³Ø©                      7410\n",
            "Ø«Ù‚Ø§ÙØ© ÙˆÙÙ†                  5841\n",
            "ØµØ­Ø©                        3458\n",
            "Ø£Ø®Ø¨Ø§Ø±                      3156\n",
            "Sport                      1213\n",
            "Politics                   1037\n",
            "Science and Technology      800\n",
            "Economic                    780\n",
            "ØªØ±ÙÙŠÙ‡                       501\n",
            "news                        414\n",
            "Social                      405\n",
            "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§                   405\n",
            "sport                       336\n",
            "Economy                     321\n",
            "middle-east                 135\n",
            "health                      103\n",
            "Science                      99\n",
            "culture                      98\n",
            "Sports                       92\n",
            "tech                         90\n",
            "entertainment                84\n",
            "politics                     68\n",
            "Tech                         48\n",
            "Technology                   39\n",
            "Religion                     36\n",
            "business                     32\n",
            "Ù…ÙˆØ³ÙŠÙ‚Ù‰                       29\n",
            "Finance                      22\n",
            "opinion                      14\n",
            "midan                        14\n",
            "science                      14\n",
            "turath                       14\n",
            "arts                         14\n",
            "family                       14\n",
            "lifestyle                    14\n",
            "Culture                      12\n",
            "Medical                       5\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "if 'Topic' in df.columns:\n",
        "    topic_counts = df['Topic'].value_counts()\n",
        "    print(topic_counts)\n",
        "else:\n",
        "    print(\"the Column Doesn't exist \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37oUMFqEwft1"
      },
      "source": [
        "### Ø¨Ø­Ø°Ù Ø§Ù„ØªÙˆØ¨ÙŠÙƒ Ø§Ù„ØºÙŠØ± Ù…Ù‡Ù…Ø© Ùˆ Ø¨Ø¹ÙŠØ¯Ø© Ø¹Ù† ÙÙƒØ±Ø© Ø§Ù„Ø¬Ø±Ø§Ø¦Ù…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uVagSS5n2Y5",
        "outputId": "21ff4cfd-8f41-4594-86ba-ef243fab6e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of rows after cleaning 16227\n",
            "saved done :  filtered_crime_dataset.csv ÙÙŠ Google Drive \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df['Topic_clean'] = df['Topic'].str.lower().str.strip()\n",
        "\n",
        "topics_to_remove = [\n",
        "    'ØªØ±ÙÙŠÙ‡', 'entertainment',\n",
        "    'Ø±ÙŠØ§Ø¶Ø©', 'sport', 'sports',\n",
        "    'Ø«Ù‚Ø§ÙØ©', 'Ø«Ù‚Ø§ÙØ© ÙˆÙÙ†', 'ÙÙ†ÙˆÙ†', 'ÙÙ†', 'culture', 'arts',\n",
        "    'Ù…ÙˆØ³ÙŠÙ‚Ù‰', 'music',\n",
        "    'ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§', 'tech', 'technology', 'science and technology', 'science',\n",
        "    'Ø§Ù‚ØªØµØ§Ø¯', 'economic', 'economy', 'finance', 'business',\n",
        "    'Ø¹Ø§Ø¦Ù„Ø©', 'family',\n",
        "    'lifestyle', 'Ù†Ù…Ø· Ø§Ù„Ø­ÙŠØ§Ø©', 'turath', 'midan','opinion'\n",
        "]\n",
        "\n",
        "df_filtered = df[~df['Topic_clean'].isin(topics_to_remove)]\n",
        "\n",
        "df_filtered = df_filtered.drop(columns=['Topic_clean'])\n",
        "\n",
        "print(\"number of rows after cleaning\", len(df_filtered))\n",
        "df_filtered['Topic'].value_counts()\n",
        "df_filtered.to_csv('/content/drive/MyDrive/filtered_dataset333.csv', index=False)\n",
        "print(\"saved done :  filtered_crime_dataset.csv ÙÙŠ Google Drive \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMAnUB5fLCf1",
        "outputId": "37712a7b-b181-4257-bf1d-c90a3d9e1dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the topic after clean we will use\n",
            "['politics' 'middle-east' 'Politics' 'Religion' 'Medical' 'Social' 'news'\n",
            " 'health' 'Ø£Ø®Ø¨Ø§Ø±' 'Ø³ÙŠØ§Ø³Ø©' 'ØµØ­Ø©']\n"
          ]
        }
      ],
      "source": [
        "print(\"the topic after clean we will use\")\n",
        "print(df_filtered['Topic'].unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdCu9o7kwls9"
      },
      "source": [
        "###  Ø¹Ù…Ù„Ù†Ø§ Ù…ÙŠØ±Ø¬ Ù„2 Ø¯Ø§ØªØ§ Ø¨Ø¯Ù†Ø§ Ù†Ø¯Ø±Ø¨ Ø¹Ù„ÙŠÙ‡Ù†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBm8n1sCyDhy",
        "outputId": "9e1e2381-595f-4b1e-8308-c63ce6da5977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rows of first dataset Index(['Label', 'Article_content'], dtype='object')\n",
            "rows of second dataset Index(['Label', 'Article_content'], dtype='object')\n",
            "number of rows after merge 18227\n",
            "  Label                                    Article_content\n",
            "0  real  Ø§Ù„Ù‚Ø¯Ø³ Ø§Ù„Ù…Ø­ØªÙ„Ø©- Ù„Ù† ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ù…Ù‚Ø¯Ø³ÙŠ Ø³Ø§Ù…ÙŠ Ø¯Ø±ÙˆÙŠØ´ ÙÙŠ ...\n",
            "1  real  Ø·Ù‡Ø±Ø§Ù†- Ù…Ù†Ø° Ø§Ù„Ù‡Ø¬ÙˆÙ… Ø§Ù„Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ù†ØµÙ„ÙŠØ© Ø§Ù„Ø¥ÙŠ...\n",
            "2  real  ØºØ§Ø¯Ø± Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ Ø±Ø§Ø¬ÙŠ ØµÙˆØ±Ø§Ù†ÙŠ Ù‚Ø·Ø§Ø¹ ØºØ²Ø© Ø±Ù...\n",
            "3  real  Ù†Ø§Ø¨Ù„Ø³- Ù„Ù„ÙŠÙˆÙ… Ø§Ù„Ø«Ø§Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§Ù„ÙŠØŒ ØªØªØ¹Ø±Ø¶ Ù‚Ø±ÙŠØ© Ø¯Ùˆ...\n",
            "4  real  Ø¨Ø±ÙŠØªÙˆØ±ÙŠØ§- ØªÙ‚Ø¯Ù…Øª Ø§Ù„Ù„Ø¬Ù†Ø© Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø© Ù„Ù„Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª ÙÙŠ ...\n",
            "Saved: merged_dataset_f.csv ÙÙŠ Google Drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/filtered_dataset333.csv')\n",
        "df2 = pd.read_csv('/content/merged_dataset12 - merged_dataset1.csv')\n",
        "\n",
        "df1 = df1.drop(columns=['Topic'])\n",
        "\n",
        "\n",
        "print(\"rows of first dataset\", df1.columns)\n",
        "print(\"rows of second dataset\", df2.columns)\n",
        "\n",
        "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "print(\"number of rows after merge\", len(merged_df))\n",
        "\n",
        "print(merged_df.head())\n",
        "merged_df.to_csv('/content/drive/MyDrive/full_dataset33333.csv', index=False)\n",
        "print(\"Saved: merged_dataset_f.csv ÙÙŠ Google Drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fwXpRMEzg3x",
        "outputId": "dbf9458f-441f-44f3-bff2-cf26c0ca6b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Label                                    Article_content\n",
            "0   real  Ø§Ù„Ù‚Ø¯Ø³ Ø§Ù„Ù…Ø­ØªÙ„Ø©- Ù„Ù† ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ù…Ù‚Ø¯Ø³ÙŠ Ø³Ø§Ù…ÙŠ Ø¯Ø±ÙˆÙŠØ´ ÙÙŠ ...\n",
            "1   real  Ø·Ù‡Ø±Ø§Ù†- Ù…Ù†Ø° Ø§Ù„Ù‡Ø¬ÙˆÙ… Ø§Ù„Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ù†ØµÙ„ÙŠØ© Ø§Ù„Ø¥ÙŠ...\n",
            "2   real  ØºØ§Ø¯Ø± Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ Ø±Ø§Ø¬ÙŠ ØµÙˆØ±Ø§Ù†ÙŠ Ù‚Ø·Ø§Ø¹ ØºØ²Ø© Ø±Ù...\n",
            "3   real  Ù†Ø§Ø¨Ù„Ø³- Ù„Ù„ÙŠÙˆÙ… Ø§Ù„Ø«Ø§Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§Ù„ÙŠØŒ ØªØªØ¹Ø±Ø¶ Ù‚Ø±ÙŠØ© Ø¯Ùˆ...\n",
            "4   real  Ø¨Ø±ÙŠØªÙˆØ±ÙŠØ§- ØªÙ‚Ø¯Ù…Øª Ø§Ù„Ù„Ø¬Ù†Ø© Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø© Ù„Ù„Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª ÙÙŠ ...\n",
            "5   real  ÙŠØ±Ù‰ Ø§Ù„Ø®Ø¨ÙŠØ± Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ ÙˆØ§Ù„Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ Ø§Ù„Ø¹Ù‚ÙŠØ¯ Ø±ÙƒÙ† Ø­Ø§Øª...\n",
            "6   real  Ø¨ØºØ¯Ø§Ø¯- ÙÙŠ Ø®Ø·ÙˆØ© Ù…ÙØ§Ø¬Ø¦Ø© ÙˆØ®Ù„Ø§Ù„ Ø£ÙŠØ§Ù… Ø¹ÙŠØ¯ Ø§Ù„ÙØ·Ø±ØŒ Ø£Ø¹...\n",
            "7   real  ØªØ±ÙƒØ² ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù†ØªÙ‚Ø§Ø¯Ø§Øª Ø§Ù„ØºØ±Ø¨ Ù„Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ Ø¹Ù„Ù‰ Ø±Ø¦ÙŠØ³ ...\n",
            "8   real  Ù„ÙˆÙ…ÙŠ- ÙÙŠ ØªØ·ÙˆØ± Ø³ÙŠØ§Ø³ÙŠ Ù…Ø«ÙŠØ± ØªØ´Ù‡Ø¯Ù‡ Ø¬Ù…Ù‡ÙˆØ±ÙŠØ© ØªÙˆØºÙˆØŒ Ù‚...\n",
            "9   real  Ø¨Ø¹Ø¯ Ø¹Ø§Ù… Ù…Ù† Ø§Ù„Ù‚ØªØ§Ù„ Ø§Ù„Ø°ÙŠ Ø§Ù†Ø¯Ù„Ø¹ ÙÙŠ Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ø³ÙˆØ¯Ø§...\n",
            "10  real  Ø¨Ø±Ù„ÙŠÙ†- Ù…Ù† Ø¬Ø¯ÙŠØ¯ ÙŠÙˆØ§Ø¬Ù‡ ØµØ­ÙÙŠÙˆÙ† Ù…ØªØ¹Ø§Ø·ÙÙˆÙ† Ù…Ø¹ Ø§Ù„Ù‚Ø¶ÙŠØ©...\n",
            "11  real  Ø§Ù„Ø­Ù‚ ÙŠÙ‚Ø§Ù„ Ø¥Ù† Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ Ø®Ø³Ø±Øª Ø§Ù„Ø­Ø±Ø¨ØŒ Ø¹Ù„Ù‰ Ø­Ø¯ ØªØ¹Ø¨ÙŠØ± ...\n",
            "12  real  Ø±ÙƒØ² Ù…Ø­Ù„Ù„ÙˆÙ† ÙˆØ®Ø¨Ø±Ø§Ø¡ Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ÙŠÙˆÙ† ÙÙŠ Ù†Ù‚Ø§Ø´Ø§ØªÙ‡Ù… Ø¨Ù‚Ù†ÙˆØ§...\n",
            "13  real  ØªØ±Ø¬Ù…Ø© ÙˆØªØ­Ø±ÙŠØ±: Ø¹Ù„ÙŠ Ø¨Ø§Ø¨ÙƒØ± Ù†Ø´Ø±Øª ØµØ­ÙŠÙØ© \"ÙˆØ§Ø´Ù†Ø·Ù† Ø¨ÙˆØ³...\n",
            "14  real  Ø¯Ø¨ÙŠØŒ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø© (CNN)-- Ø£Ø¹Ù„Ù†Øª Ù‡ÙŠ...\n",
            "15  real  (CNN)-- Ø¯Ø¹Ø§ Ø§Ù„Ø¨ÙŠØª Ø§Ù„Ø£Ø¨ÙŠØ¶ Ø¥ÙŠØ±Ø§Ù† Ø¥Ù„Ù‰ Ø§Ù„Ø¥ÙØ±Ø§Ø¬ Ø¹Ù† ...\n",
            "16  real  Ø§Ù„Ù‚Ø¯Ø³ (CNN)-- Ù‚Ø§Ù„ Ø´Ù‡ÙˆØ¯ Ø¹ÙŠØ§Ù† Ù„Ø´Ø¨ÙƒØ© CNNØŒ Ø§Ù„Ø³Ø¨ØªØŒ ...\n",
            "17  real  (CNN)-- Ø´Ù‡Ø¯ ÙØ±ÙŠÙ‚ CNN Ø¯Ø®ÙˆÙ„ Ø¹Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø³ØªÙˆØ·Ù†ÙŠÙ† Ø¥Ù„Ù‰...\n",
            "18  real  Ø¯Ø¨ÙŠØŒ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø© (CNN)-- Ø£Ø¹Ù„Ù† Ø§Ù„Ø£...\n",
            "19  real  \\nØ§Ù„Ù‚Ø¯Ø³ (CNN)-- Ù‚Ø§Ù„ Ø§Ù„Ù…ØªØ­Ø¯Ø« Ø¨Ø§Ø³Ù… Ø§Ù„Ø¬ÙŠØ´ Ø§Ù„Ø¥Ø³Ø±Ø§Ø¦...\n"
          ]
        }
      ],
      "source": [
        "print(merged_df.head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0puOKYLww14"
      },
      "source": [
        "### Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù‚ÙŠÙ… Ø§Ù„ NULL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-h_4HVfztGl",
        "outputId": "3aec8137-d6e2-4ab2-9877-179d2fea226d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label              0\n",
            "Article_content    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(merged_df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ”¢ Total rows:\", len(merged_df))\n",
        "print(\"ğŸ§¾ Unique 'Article_content':\", merged_df['Article_content'].nunique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIwV6bZMO8kx",
        "outputId": "35002015-f59e-4627-fb83-2a820978df90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¢ Total rows: 18227\n",
            "ğŸ§¾ Unique 'Article_content': 4205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPn2WM2Jw_iJ"
      },
      "source": [
        "### Ø¨ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ ØªØ´ÙƒÙŠÙ„ + Ø±Ù…ÙˆØ² ØºØ±ÙŠØ¨Ø© Ùˆ Ø¥Ù„Ø®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lYxUJOo3EbY",
        "outputId": "1c83c21b-8093-4031-da0e-17a4eb002833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Atricles that has tashkeel  7732\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def has_arabic_tashkeel(text):\n",
        "    tashkeel_pattern = r'[\\u064B-\\u0652]'\n",
        "    return bool(re.search(tashkeel_pattern, str(text)))\n",
        "\n",
        "tashkeel_flags = merged_df['Article_content'].apply(has_arabic_tashkeel)\n",
        "print(\"Number of Atricles that has tashkeel \", tashkeel_flags.sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhxGLqal4a5e",
        "outputId": "c78de9cc-31d4-4681-bf4b-bec24318bfc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Atricles that has strange_symbols 1128\n"
          ]
        }
      ],
      "source": [
        "def has_strange_symbols(text):\n",
        "    return bool(re.search(r'[ØŸ?!@#$%^&*~<>+=_]', str(text)))\n",
        "\n",
        "symbol_flags = merged_df['Article_content'].apply(has_strange_symbols)\n",
        "print(\"Number of Atricles that has strange_symbols\", symbol_flags.sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXAhP9cr4kxB",
        "outputId": "a63c3978-dd4a-4c97-8dec-4ce1f6d9f9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Empty or missing rows in 'Article_content': 130\n"
          ]
        }
      ],
      "source": [
        "empty_rows = merged_df['Article_content'].isna() | (merged_df['Article_content'].astype(str).str.strip() == '')\n",
        "print(\" Empty or missing rows in 'Article_content':\", empty_rows.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixKe9CY846SG",
        "outputId": "3361d8a1-d4f6-4c28-b597-0dd60c25d1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Articles has many digits 837\n"
          ]
        }
      ],
      "source": [
        "has_many_digits = merged_df['Article_content'].apply(lambda x: len(re.findall(r'\\d', str(x))) > 5)\n",
        "print(\"Articles has many digits\", has_many_digits.sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcl2jKxX5AZL",
        "outputId": "6f34fa11-e24e-4edb-a493-00a74a2ca44f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Atrticles has english words 1565\n"
          ]
        }
      ],
      "source": [
        "has_english_words = merged_df['Article_content'].apply(lambda x: bool(re.search(r'[a-zA-Z]', str(x))))\n",
        "print(\"Atrticles has english words\", has_english_words.sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaHG0WUAxHWE"
      },
      "source": [
        "### Ø¨Ø¹Ù…Ù„ preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0Wpq7zNI5EfK"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_article(text):\n",
        "    text = str(text)\n",
        "\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "    # Ø­Ø°Ù Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    text = re.sub(r'#\\S+|@\\S+', '', text)\n",
        "\n",
        "    text = re.sub(r'\\[\\]|\\bcontent not available\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(r'[â€¢â–ªï¸â€”â€“â€¦Â·]', '', text)\n",
        "\n",
        "    common_phrases = [\n",
        "        r'Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ù‡Ù†Ø§\\s*:', r'Ø§Ù‚Ø±Ø£ Ø£ÙŠØ¶Ù‹Ø§', r'Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ø²ÙŠØ¯',\n",
        "        r'Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„', r'ØªØ§Ø¨Ø¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„'\n",
        "    ]\n",
        "    for phrase in common_phrases:\n",
        "        text = re.sub(phrase, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(r'[.:;()\\[\\]{}\"\\'Â«Â»<>\\-]', '', text)\n",
        "    text = re.sub(r'\\\\n|\\\\r|\\n|\\r', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6ehck1Bs9zAL"
      },
      "outputs": [],
      "source": [
        "merged_df['Article_content'] = merged_df['Article_content'].apply(clean_article)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j37laDoiQi7M",
        "outputId": "408b9854-f5c2-49e3-9043-54a0b677bc6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of articles After Remove Empty Articles 18097\n"
          ]
        }
      ],
      "source": [
        "cleaned_df = merged_df[merged_df['Article_content'].str.strip() != '']\n",
        "\n",
        "print(\"Number of articles After Remove Empty Articles\", len(cleaned_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3HET-ryOird",
        "outputId": "c9833b1d-5e91-4804-d3cd-b34e02e3e865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty Spaces or articles  0\n"
          ]
        }
      ],
      "source": [
        "empty_texts = cleaned_df['Article_content'].apply(lambda x: str(x).strip() == '')\n",
        "print(\"Empty Spaces or articles \", empty_texts.sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHmAmujMOy0H"
      },
      "outputs": [],
      "source": [
        "cleaned_df.to_csv('/content/drive/MyDrive/full_dataset5.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p1i-y_fxOZQ"
      },
      "source": [
        "### Ù…Ù†Ø¨Ù„Ø´ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯Ù„ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fdz7lTipxXOx"
      },
      "source": [
        "### ÙˆØ¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù‚Ù‚ Ø¨ÙƒØªØ´Ù Ù…Ù† Ø¹Ø¯Ù… ØªÙˆØ§Ø²Ù† Ø§Ù„Ø¯Ø§ØªØ§\n",
        "### Ø§Ù„ real Ø§ØµØºØ± Ø¨ÙƒØªÙŠØ± Ù…Ù† fake\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWAXFx5GZ-vg",
        "outputId": "eccb2454-e3de-498f-fc3b-791ea820d966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ],
      "source": [
        "!pip install deep-translator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjc6aY7jgWeX"
      },
      "source": [
        "### Ù‚Ø±Ø±Ù†Ø§ Ø¨Ø¹Ø¯Ø§ Ù†Ø¹Ù…Ù„ augmentation for real t0 5000\n",
        "### resample for fake to 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2dMgZsXCuwR",
        "outputId": "8c6b52e6-e882-4a38-8ea9-d6aec961fdfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:\n",
            "Label\n",
            "fake    5000\n",
            "real    5000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ğŸŸ¢ Ø¹ÙŠÙ†Ø© Ù…Ù† real:\n",
            "  Label                                    Article_content\n",
            "1  real  ÙŠÙ‚ÙˆÙ„ Ø±Ø¦ÙŠØ³ ÙƒØ§Ø²Ø§Ø®Ø³ØªØ§Ù† Ø¥Ù† Ù‚ÙˆØ§Øª Ù…Ù†Ø¸Ù…Ø© Ù…Ø¹Ø§Ù‡Ø¯Ø© Ø§Ù„Ø£Ù…Ù†...\n",
            "2  real  Ø¨Ø§Ø±Ø²Ø§Ù†ÙŠ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…Ø´ØªØ±Ùƒ Ø¨ÙŠÙ† Ø¨ØºØ¯Ø§Ø¯ ÙˆØ§Ø±Ø¨ÙŠÙ„ Ø§Ø³Ù‡Ù… Ø¨Ø®...\n",
            "\n",
            "ğŸ”´ Ø¹ÙŠÙ†Ø© Ù…Ù† fake:\n",
            "  Label                                    Article_content\n",
            "0  fake  Ø¨Ø´Ø§Ø± Ø§Ù„Ø£Ø³Ø¯ Ù„Ù… ÙŠØªÙˆØ¹Ø¯ Ø¨ØªØ¬Ø±ÙŠØ¯ Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† ÙÙŠ ØªØ±ÙƒÙŠØ§ Ù…...\n",
            "5  fake  Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…ÙØ¨Ø±Ùƒ ÙˆÙ„Ù… ÙŠØµÙØ¹ Ø´Ø§Ø¨ ÙƒØ±Ø¯ÙŠ Ø¯ÙˆÙ†Ø§Ù„Ø¯ ØªØ±Ø§Ù…Ø¨ Ø§...\n"
          ]
        }
      ],
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "import pandas as pd\n",
        "\n",
        "def back_translate_arabic(text):\n",
        "    try:\n",
        "        translated_en = GoogleTranslator(source='ar', target='en').translate(text)\n",
        "        translated_back = GoogleTranslator(source='en', target='ar').translate(translated_en)\n",
        "        return translated_back\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "real_df = cleaned_df[cleaned_df['Label'] == 'real']\n",
        "real_count = len(real_df)\n",
        "needed_augmented = 5000 - real_count\n",
        "\n",
        "sample_for_aug = real_df.sample(n=needed_augmented, replace=True, random_state=42)\n",
        "augmented_texts = sample_for_aug['Article_content'].apply(back_translate_arabic)\n",
        "augmented_real_df = sample_for_aug.copy()\n",
        "augmented_real_df['Article_content'] = augmented_texts\n",
        "\n",
        "final_real_df = pd.concat([real_df, augmented_real_df]).reset_index(drop=True)\n",
        "\n",
        "fake_df = cleaned_df[cleaned_df['Label'] == 'fake'].sample(n=5000, random_state=42).reset_index(drop=True)\n",
        "balanced_df = pd.concat([final_real_df, fake_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"\\n Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:\")\n",
        "print(balanced_df['Label'].value_counts())\n",
        "print(\"\\nğŸŸ¢ Ø¹ÙŠÙ†Ø© Ù…Ù† real:\")\n",
        "print(balanced_df[balanced_df['Label'] == 'real'].head(2))\n",
        "print(\"\\nğŸ”´ Ø¹ÙŠÙ†Ø© Ù…Ù† fake:\")\n",
        "print(balanced_df[balanced_df['Label'] == 'fake'].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbkSKeeHyAgH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1CefUlZL9Gi"
      },
      "outputs": [],
      "source": [
        "X = balanced_df['Article_content'].astype(str)\n",
        "y = balanced_df['Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xDtV4C8yY5v"
      },
      "outputs": [],
      "source": [
        "label_enc = LabelEncoder()\n",
        "y_encoded = label_enc.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w7xanIhycTb"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeWX1oDwyc2r"
      },
      "outputs": [],
      "source": [
        "max_words = 10000\n",
        "max_len = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h7XgQyzykBo"
      },
      "outputs": [],
      "source": [
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7B_rwU1ymG9",
        "outputId": "4b710a1e-f78a-4aab-e04c-8c5560525f66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # binary classification\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ia_6jI2yomX"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNsfPRcSyqtr",
        "outputId": "554a7908-ff05-48a5-bd98-a2d01d59ee8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.5560 - loss: 0.6685 - val_accuracy: 0.6450 - val_loss: 0.5649\n",
            "Epoch 2/5\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.7775 - loss: 0.4814 - val_accuracy: 0.9500 - val_loss: 0.2667\n",
            "Epoch 3/5\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8977 - loss: 0.2573 - val_accuracy: 0.9312 - val_loss: 0.1981\n",
            "Epoch 4/5\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9112 - loss: 0.2169 - val_accuracy: 0.7925 - val_loss: 0.3441\n",
            "Epoch 5/5\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.9346 - loss: 0.1612 - val_accuracy: 0.9112 - val_loss: 0.1981\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "\n",
            "ğŸ“Š Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.86      0.99      0.92      1000\n",
            "        real       0.99      0.84      0.91      1000\n",
            "\n",
            "    accuracy                           0.92      2000\n",
            "   macro avg       0.93      0.92      0.92      2000\n",
            "weighted avg       0.93      0.92      0.92      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(\"\\nğŸ“Š Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_enc.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLPYMR4gP-DD",
        "outputId": "88a7baab-73cd-441a-e386-e226b20b5588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9480 - loss: 0.1355 - val_accuracy: 0.9388 - val_loss: 0.1639\n",
            "Epoch 2/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.9616 - loss: 0.1029 - val_accuracy: 0.9513 - val_loss: 0.1475\n",
            "Epoch 3/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9602 - loss: 0.1037 - val_accuracy: 0.9463 - val_loss: 0.1680\n",
            "Epoch 4/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.9507 - loss: 0.1231 - val_accuracy: 0.9550 - val_loss: 0.1405\n",
            "Epoch 5/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.9694 - loss: 0.0778 - val_accuracy: 0.8700 - val_loss: 0.2723\n",
            "Epoch 6/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.9558 - loss: 0.1152 - val_accuracy: 0.9600 - val_loss: 0.1347\n",
            "Epoch 7/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9772 - loss: 0.0650 - val_accuracy: 0.9563 - val_loss: 0.1590\n",
            "Epoch 8/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9786 - loss: 0.0627 - val_accuracy: 0.9563 - val_loss: 0.1480\n",
            "Epoch 9/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.9759 - loss: 0.0654 - val_accuracy: 0.9588 - val_loss: 0.1463\n",
            "Epoch 10/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9737 - loss: 0.0674 - val_accuracy: 0.9600 - val_loss: 0.1241\n",
            "\u001b[1m63/63\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "ğŸ“Š Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.97      0.95      0.96      1000\n",
            "        real       0.95      0.97      0.96      1000\n",
            "\n",
            "    accuracy                           0.96      2000\n",
            "   macro avg       0.96      0.96      0.96      2000\n",
            "weighted avg       0.96      0.96      0.96      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(\"\\nğŸ“Š Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_enc.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8LsP8PqP_j4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiawnIdYQWEU"
      },
      "source": [
        "### Ù…Ù†Ø¹Ù…Ù„ Ø±ÙŠ Ø³Ø§Ù…Ø¨Ù„ fake ---> 10k\n",
        "### ÙˆÙ…Ù†Ø¹Ù…Ù„ Augmentation for real --> 10k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW-lGEPZQqrt",
        "outputId": "ad398553-b0af-4b0e-be77-78388b84cb6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Final Balance \n",
            "Label\n",
            "fake    10000\n",
            "real    10000\n",
            "Name: count, dtype: int64\n",
            "\n",
            " real:\n",
            "  Label                                    Article_content\n",
            "1  real  Ù…Ù‚Ø§Ø·Ø¹Ø© ØªØ³Ø¬ÙŠÙ„ ÙˆÙØ§Ø© ÙˆØ§Ø­Ø¯Ø© Ùˆ 851 Ø¹Ø¯ÙˆÙ‰ Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ø§Ù„...\n",
            "2  real  Ø§Ù„Ù…Ø§Ù„ÙŠÙƒÙŠ ØŒ Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ÙŠ Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡ ØŒ Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ø§Ø³ØªÙ‡Ø¯Ø§...\n",
            "\n",
            " fake:\n",
            "  Label                                    Article_content\n",
            "0  fake  Ù„Ù… ÙŠØªØ±Ùƒ Ø·Ø¨ÙŠØ¨ Ø£Ù„Ù…Ø§Ù†ÙŠ Ù‚Ø¨Ù„ Ø§Ù†ØªØ­Ø§Ø±Ù‡ Ø±Ø³Ø§Ù„Ø© Ø¨Ø£Ù†Ù‘ Ù„Ù‚Ø§...\n",
            "4  fake  Ø§Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…Ù†Ø³ÙˆØ¨ Ù„Ù…ÙŠØ¯Ù„ Ø¥ÙŠØ³Øª Ù…ÙˆÙ†ÙŠØªÙˆØ± Ø¹Ù† ØªØ­ÙˆÙŠÙ„ Ø³Ø¹...\n"
          ]
        }
      ],
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "import pandas as pd\n",
        "\n",
        "def back_translate_arabic(text):\n",
        "    try:\n",
        "        translated_en = GoogleTranslator(source='ar', target='en').translate(text)\n",
        "        translated_back = GoogleTranslator(source='en', target='ar').translate(translated_en)\n",
        "        return translated_back\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "real_df = cleaned_df[cleaned_df['Label'] == 'real']\n",
        "real_count = len(real_df)\n",
        "needed_augmented = 10000 - real_count\n",
        "sample_for_aug = real_df.sample(n=needed_augmented, replace=True, random_state=42)\n",
        "augmented_texts = sample_for_aug['Article_content'].apply(back_translate_arabic)\n",
        "augmented_real_df = sample_for_aug.copy()\n",
        "augmented_real_df['Article_content'] = augmented_texts\n",
        "\n",
        "final_real_df = pd.concat([real_df, augmented_real_df]).reset_index(drop=True)\n",
        "\n",
        "fake_df = cleaned_df[cleaned_df['Label'] == 'fake'].sample(n=10000, random_state=42).reset_index(drop=True)\n",
        "balanced_df = pd.concat([final_real_df, fake_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"\\n Final Balance \")\n",
        "print(balanced_df['Label'].value_counts())\n",
        "print(\"\\n real:\")\n",
        "print(balanced_df[balanced_df['Label'] == 'real'].head(2))\n",
        "print(\"\\n fake:\")\n",
        "print(balanced_df[balanced_df['Label'] == 'fake'].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O26Z7DkwQwLh"
      },
      "outputs": [],
      "source": [
        "X = balanced_df['Article_content'].astype(str)\n",
        "y = balanced_df['Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAB_ZFgmaU4Y"
      },
      "outputs": [],
      "source": [
        "label_enc = LabelEncoder()\n",
        "y_encoded = label_enc.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg41ISEaaaKW"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhitozNuahVp"
      },
      "outputs": [],
      "source": [
        "max_words = 10000\n",
        "max_len = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v79DA1wQajxn"
      },
      "outputs": [],
      "source": [
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSgT53OGampf"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # binary classification\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB_H0nPOapGO"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOhaaBeqap2W",
        "outputId": "adca1819-ac86-4343-83fa-01c762d1a2ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6161 - loss: 0.6264 - val_accuracy: 0.8856 - val_loss: 0.2993\n",
            "Epoch 2/5\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8992 - loss: 0.2698 - val_accuracy: 0.8825 - val_loss: 0.2083\n",
            "Epoch 3/5\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9350 - loss: 0.1638 - val_accuracy: 0.9444 - val_loss: 0.1445\n",
            "Epoch 4/5\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9527 - loss: 0.1257 - val_accuracy: 0.8894 - val_loss: 0.2215\n",
            "Epoch 5/5\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9582 - loss: 0.1162 - val_accuracy: 0.9669 - val_loss: 0.1011\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\n",
            " Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.97      0.97      0.97      2000\n",
            "        real       0.97      0.97      0.97      2000\n",
            "\n",
            "    accuracy                           0.97      4000\n",
            "   macro avg       0.97      0.97      0.97      4000\n",
            "weighted avg       0.97      0.97      0.97      4000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(\"\\n Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_enc.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H91lXkQLeXsn",
        "outputId": "5f91f94e-8401-4ca4-faf5-8c20b2a1bac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9659 - loss: 0.0947 - val_accuracy: 0.9675 - val_loss: 0.0956\n",
            "Epoch 2/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9727 - loss: 0.0777 - val_accuracy: 0.9706 - val_loss: 0.0889\n",
            "Epoch 3/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9713 - loss: 0.0753 - val_accuracy: 0.9619 - val_loss: 0.1048\n",
            "Epoch 4/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9725 - loss: 0.0737 - val_accuracy: 0.9731 - val_loss: 0.0808\n",
            "Epoch 5/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9741 - loss: 0.0676 - val_accuracy: 0.9762 - val_loss: 0.0740\n",
            "Epoch 6/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9805 - loss: 0.0528 - val_accuracy: 0.9619 - val_loss: 0.1234\n",
            "Epoch 7/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9832 - loss: 0.0470 - val_accuracy: 0.9787 - val_loss: 0.0714\n",
            "Epoch 8/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9747 - loss: 0.0655 - val_accuracy: 0.9787 - val_loss: 0.0778\n",
            "Epoch 9/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9841 - loss: 0.0444 - val_accuracy: 0.9681 - val_loss: 0.1097\n",
            "Epoch 10/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9871 - loss: 0.0374 - val_accuracy: 0.9606 - val_loss: 0.1639\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\n",
            " Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       1.00      0.93      0.96      2000\n",
            "        real       0.93      1.00      0.96      2000\n",
            "\n",
            "    accuracy                           0.96      4000\n",
            "   macro avg       0.97      0.96      0.96      4000\n",
            "weighted avg       0.97      0.96      0.96      4000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(\"\\n Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_enc.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t5p-3COitpQ",
        "outputId": "8530c961-887b-49f7-f3ff-18aa5620fcfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer saved as .pkl successfully.\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(tokenizer, '/content/drive/MyDrive/tokenizer2.pkl')\n",
        "\n",
        "print(\"Tokenizer saved as .pkl successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7LOend0jwh1",
        "outputId": "d9d847fc-2164-42cb-e73e-5d7f0ad753aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('real', 100.0)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "def predict_article_dl(text):\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
        "    prob = model.predict(padded)[0][0]\n",
        "    cls = int(prob > 0.5)\n",
        "    label = label_enc.inverse_transform([cls])[0]\n",
        "    confidence = prob if cls == 1 else 1 - prob\n",
        "    return label, confidence\n",
        "\n",
        "sample_text = \"Ø£Ø¹Ù„Ù†Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø¹Ù† Ø¥Ù„Ù‚Ø§Ø¡ Ø§Ù„Ù‚Ø¨Ø¶ Ø¹Ù„Ù‰ Ø´Ø¨ÙƒØ© ØªÙ…ØªÙ‡Ù† Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ÙÙŠ Ø¹Ø¯Ø© Ù…Ù†Ø§Ø·Ù‚.\"\n",
        "predicted_label, confidence = predict_article_dl(sample_text)\n",
        "predicted_label, round(confidence * 100, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h-xiQccllwM"
      },
      "outputs": [],
      "source": [
        "%reset -f\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}